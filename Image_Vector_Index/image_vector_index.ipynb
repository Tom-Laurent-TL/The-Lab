{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8016f4e",
   "metadata": {},
   "source": [
    "# Image Vector Indexing and Text-to-Image Retrieval\n",
    "\n",
    "This notebook will:\n",
    "1. Load images from a directory  \n",
    "2. Compute image embeddings using a pretrained vision model  \n",
    "3. Build a FAISS index over those embeddings  \n",
    "4. Encode a text query and retrieve the top-k most similar images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Model Initialization\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
    "\n",
    "# Vision model to embed images\n",
    "processor = AutoImageProcessor.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\")\n",
    "vision_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\", trust_remote_code=True)\n",
    "vision_model.eval()\n",
    "\n",
    "# Text model to embed queries\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "text_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "text_model.eval()\n",
    "\n",
    "# Mean pooling helper for text embeddings\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = torch.sum(token_embeddings * mask_expanded, dim=1)\n",
    "    counts = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return summed / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0849aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 images, embedding dim = 768\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Images and Compute Embeddings\n",
    "IMAGE_DIR = \"images\"  # adjust path as needed\n",
    "image_paths = sorted(glob.glob(os.path.join(IMAGE_DIR, \"*.[jp][pn]g\")))\n",
    "\n",
    "image_embeddings = []\n",
    "for path in image_paths:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = vision_model(**inputs).last_hidden_state\n",
    "    emb = F.normalize(out[:, 0], p=2, dim=1)  # take [CLS] token\n",
    "    image_embeddings.append(emb.cpu().numpy().astype(\"float32\"))\n",
    "\n",
    "# Stack into array of shape (N, D)\n",
    "image_embeddings = np.vstack(image_embeddings)\n",
    "print(f\"Loaded {len(image_paths)} images, embedding dim = {image_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d28f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 6 vectors\n"
     ]
    }
   ],
   "source": [
    "# 3. Build FAISS Index\n",
    "dim = image_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(image_embeddings)\n",
    "print(f\"FAISS index contains {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae06b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Text-to-Image Search Function\n",
    "def search_images_by_text(query, k=5):\n",
    "    # Encode text\n",
    "    encoded = tokenizer([query], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        txt_out = text_model(**encoded)\n",
    "    txt_emb = mean_pooling(txt_out, encoded[\"attention_mask\"])\n",
    "    txt_emb = F.layer_norm(txt_emb, normalized_shape=(txt_emb.shape[1],))\n",
    "    txt_emb = F.normalize(txt_emb, p=2, dim=1).cpu().numpy().astype(\"float32\")\n",
    "    \n",
    "    # Search FAISS\n",
    "    D, I = index.search(txt_emb, k)\n",
    "    results = [(image_paths[idx], float(D[0][i])) for i, idx in enumerate(I[0])]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5277f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 matches for query: 'winter village with snow'\n",
      "\n",
      "1. images\\winter-village-scene-snowy-night-town.jpg (L2 distance = 1.7919)\n",
      "2. images\\winter_town.png (L2 distance = 1.8249)\n",
      "3. images\\snowman.jpg (L2 distance = 1.8722)\n",
      "4. images\\snow_forest.jpg (L2 distance = 1.8852)\n",
      "5. images\\beach-wave-sunset-coast-palm-tree-scenery.jpg (L2 distance = 1.9761)\n"
     ]
    }
   ],
   "source": [
    "# 5. Run an Example Query\n",
    "query = \"winter village with snow\"\n",
    "top_results = search_images_by_text(query, k=5)\n",
    "\n",
    "print(f\"Top-5 matches for query: '{query}'\\n\")\n",
    "for rank, (path, dist) in enumerate(top_results, start=1):\n",
    "    print(f\"{rank}. {path} (L2 distance = {dist:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
