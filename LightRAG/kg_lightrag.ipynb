{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154953fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lightrag-hku\n",
    "# %pip install pipmaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1521c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.utils import EmbeddingFunc, setup_logger\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.llm.ollama import ollama_embed\n",
    "from lightrag.llm.azure_openai import azure_openai_complete, azure_openai_complete_if_cache\n",
    "from mistralai import Mistral\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501012c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "COMPLETION_MODEL = os.getenv(\"MISTRAL_MODEL\", \"mistral-small-latest\")\n",
    "MISTRAL_EMBED_MODEL = os.getenv(\"MISTRAL_EMBED_MODEL\", \"mistral-embed\")\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\" # for ollama-embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mistral_client = Mistral(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd01441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding function using chunked calls ---\n",
    "async def get_mistral_embeddings(texts: list[str], chunk_size: int = 50) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Async wrapper: compute embeddings via Mistral SDK in chunks, each call offloaded\n",
    "    to a thread so as not to block the event loop.\n",
    "    \"\"\"\n",
    "    embeddings: list[list[float]] = []\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        chunk = texts[i : i + chunk_size]\n",
    "        # run the blocking SDK call in a thread\n",
    "        resp = await asyncio.to_thread(\n",
    "            mistral_client.embeddings.create,\n",
    "            model=MISTRAL_EMBED_MODEL,\n",
    "            inputs=chunk\n",
    "        )\n",
    "        embeddings.extend([d.embedding for d in resp.data])\n",
    "    return embeddings\n",
    "\n",
    "# --- Async wrapper for LLM using Mistral SDK ---\n",
    "async def mistral_llm(prompt: str, system_prompt: str = None, history_messages: list = None, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Async wrapper that uses Mistral's official SDK in a thread pool.\n",
    "    \"\"\"\n",
    "    # Build chat messages\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt or \"You are a helpful assistant.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    def call_sdk():\n",
    "        resp = mistral_client.chat.complete(model=COMPLETION_MODEL, messages=messages)\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    # Delegate blocking call to thread\n",
    "    return await asyncio.to_thread(call_sdk)\n",
    "\n",
    "async def initialize_rag(working_dir: str):\n",
    "    \"\"\"\n",
    "    Initializes the LightRAG instance with Mistral embeddings and LLM.\n",
    "    \"\"\"\n",
    "    setup_logger(\"lightrag\", level=\"INFO\")\n",
    "    \n",
    "    rag = LightRAG(\n",
    "        working_dir=working_dir,\n",
    "        # embedding_func=EmbeddingFunc(\n",
    "        #     func=get_mistral_embeddings,\n",
    "        #     embedding_dim=1024,\n",
    "        #     max_token_size=8192\n",
    "        # ),\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=768,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: ollama_embed(\n",
    "                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n",
    "            ),\n",
    "        ),\n",
    "        # llm_model_func=mistral_llm,\n",
    "        llm_model_func=azure_openai_complete,\n",
    "        # enable_llm_cache=False,\n",
    "        # enable_llm_cache_for_entity_extract=False,\n",
    "    )\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    return rag\n",
    "\n",
    "async def load_rag(working_dir: str):\n",
    "    # (re-)create the LightRAG object with the same config you used originally\n",
    "    setup_logger(\"lightrag\", level=\"INFO\")\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=working_dir,\n",
    "        # embedding_func=EmbeddingFunc(\n",
    "        #     func=get_mistral_embeddings,\n",
    "        #     embedding_dim=1024,\n",
    "        #     max_token_size=8192\n",
    "        # ),\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=768,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: ollama_embed(\n",
    "                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n",
    "            ),\n",
    "        ),\n",
    "        llm_model_func=mistral_llm,\n",
    "        # enable_llm_cache=False,\n",
    "        # enable_llm_cache_for_entity_extract=False,\n",
    "    )\n",
    "\n",
    "    # this will *load* your existing KV/Vector/Graph stores instead of recreating them\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16114eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & environment checks\n",
    "WORK_DIR = \"./rag_storage\"\n",
    "\n",
    "# Initialize a new RAG\n",
    "rag = await initialize_rag(WORK_DIR)\n",
    "\n",
    "# Load an existing RAG\n",
    "# rag = await load_rag(WORK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = list(Path(\"../../knowledge_extraction/txt/raw\").glob(\"*.txt\"))\n",
    "txt_files = [str(f) for f in txt_files if f.is_file()]\n",
    "\n",
    "text_contents = []\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_contents.append(f.read())\n",
    "        \n",
    "\n",
    "source_filepaths = [str(f).replace(\"..\\\\\", \"\").replace(\"../\", \"\") for f in txt_files]\n",
    "source_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e06a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.insert(input=text_contents, file_paths=source_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"comment faire une esquisse dans catia ?\"\n",
    "response = rag.query(\n",
    "    question,\n",
    "    param=QueryParam(mode=\"hybrid\", top_k=20)  # hybrid local+global retrieval\n",
    ")\n",
    "\n",
    "print(f\"Q: {question}\\nA: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure storages are properly closed\n",
    "# await rag.finalize_storages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
